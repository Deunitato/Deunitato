---
layout: post
title: 'CS2040s: Hashing'
date: '2019-09-16T22:29:00.001-07:00'
author: Charlotte Deunitato
tags: 
modified_time: '2019-10-22T03:31:25.734-07:00'
thumbnail: https://1.bp.blogspot.com/-wPYAzZBOxFw/Xa7PHWPfIMI/AAAAAAAACNs/vRv-YC5X-gUjdkVm5t3RITXz5NLaPb1cgCLcBGAsYHQ/s72-c/1.PNG
blogger_id: tag:blogger.com,1999:blog-4520250687931855860.post-1247065437205110360
blogger_orig_url: https://nusmods.blogspot.com/2019/09/cs2040s-hashing.html
---

<h2>Hashing</h2><div>Problem: Find me a thief.</div><div>Want to look through a database of criminals based on fingerprints. Each lookup should be very fast.</div><div>We need to look for an ADT.</div><div><br /></div><h3>Dictionary ADT</h3><div>Operation with k = key and v = value</div><div>But it is not ordered</div><div><br /></div><h3>Implementation using Data Structure</h3><div>Insert Time:</div><div>Unordered Array/Linked List: <b>O(1)</b></div><div>Ordered Array/Linked List: <b>O(n)</b></div><div>Balance Binary Tree (AVL): <b>O(logn)</b></div><div><br /></div><div>Average search time:</div><div><div>Unordered Array/Linked List: <b>O(n)</b></div><div>Ordered Array/Linked List: <b>O(logn)</b></div><div>Balance Binary Tree (AVL): <b>O(logn)</b></div><div><br />Insert [61 week 9]</div><h4>But Hashing is O(1) search time..?</h4><div><b><span style="color: red;">Hash table does not use order comparisons. (The fastest order comparisons is O(nlogn))</span></b></div><div><br /></div><div>HashTable:</div><div>Insert - <b>O(1)</b></div><div>Avg Search time - <b>O(1)</b></div><div>Avg Max/min - <b>O(n)</b></div><div>Avg Floor/Ceiling - <b>O(n)</b></div><div><br /></div><h3>Hash Tables</h3><div>Insert and search is O(1)</div><div>But its not that good.&nbsp;</div><div>1. It takes up too much space, (2^64-1)</div><div>2. If keys are not integer</div></div><h3><b>Chaining (Separate Chaining)</b></h3><div>If there's collision, we will add it to a linked list.<br />This is to store multiple values in a bucket</div><div><br /></div><div><b><span style="color: red;">The worst case space complexity for hash table with separate chaining</span></b></div><div><b><span style="color: red;">=&gt; O(m+n)</span></b></div><div>-&gt; n is linked list worst case, m is worse case for filling up everything in hashtable</div><div>-&gt; mn is not possible because there are only n keys</div><div><br /></div><div><b><span style="color: red;">Inserting takes O(1) time</span></b></div><div>-&gt; Hashing key takes O(1)</div><div>-&gt; Insert to bucket takes O(1)</div><div>-&gt; Insert to linked list is O(1)</div><div><br /></div><div><b><span style="color: red;">Searching takes O(n) time</span></b></div><div>-&gt; Accessing the hash tables takes O(1)</div><div>-&gt; Searching linked list is O(n)</div><div><br /></div><h4><u>Simple uniform hashing assumption</u></h4><div>There is a low probability of collision</div><div>- Optimistic assumption</div><div>- Every key is likely map to every bucket</div><div>- Keys are map independently (Random cases and collision is look over)</div><div><br /></div><div><b><span style="color: red;">Average Search Time under (Simple uniform hashing assumption) is O(1)</span></b></div><div><b><span style="color: red;"><br /></span></b></div><div>Expected search time = 1 (Hashing + array access) + expected # items per bucket (Linked List traversal)</div><div><br /></div><div><b>Expected Maximum cost of search for chaining is O(logn) [Extra]</b></div><div><br /></div><div><b>Chaining With other Data structures</b></div><div><br /></div><div>AVL is O(logn) but have a lot of overhead (Memory + computational time) for small number of items</div><div><br /></div><h2>Open Addressing</h2><div>When collide, probe until can find an empty slot.<br /><br /><div><h3>Load Factor</h3></div><div>Load factor = Number of keys in the hashtable/ Size of hashtable</div><div><br /></div><div>Load factor determines how much of the table have been use.&nbsp;</div><div><br /></div></div><h3>Linear Probing</h3><div>Keep checking next bucket till find an empty slot.<br /><br /><u>Index i = (baseadd + step *1) mod m</u></div><div><br /></div><div><u>Searching</u></div><div><u><br /></u></div><div>Keep probing till we find a key match, we stop when we hit a null</div><div><br /></div><div><u>Deleting</u></div><div><br /></div><div>-&gt; When a new key is inserted, the probability of it being under the big cluster is bigger.</div><div><br /></div><div>The bigger the table becomes the worst the efficiency is.<br /><br /></div><div><b><span style="color: red;">With linear probing, big cluster are more likely to form</span></b></div><h3>Quadratic Probing</h3><div>Squaring the steps. If I cannot insert in the next key, I will jump ^2 steps. This is to prevent clustering.<br /><u>Index i = (baseadd + step ^2) mod m</u><br /><u><br /></u><b><span style="color: red;">Problem with Quadratic is that it will cause secondary clustering</span></b></div><div><br /></div><h4>Secondary Clustering</h4><div>If two keys have same probe position, their prob sequences will be the same.</div><div>Clustering around different points</div><div><br /></div><div><b><span style="color: red;">There are m probe sequences</span></b><br /><br /></div><div><b>Theory: If load factor is 1/2 and m is prime, then empty slot will always be found</b><br /><br />Prove by Contradiction:<br />Consider two probe locations,<br />step x and y<br />where x &lt; y &lt; m/2<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-wPYAzZBOxFw/Xa7PHWPfIMI/AAAAAAAACNs/vRv-YC5X-gUjdkVm5t3RITXz5NLaPb1cgCLcBGAsYHQ/s1600/1.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="182" data-original-width="736" src="https://1.bp.blogspot.com/-wPYAzZBOxFw/Xa7PHWPfIMI/AAAAAAAACNs/vRv-YC5X-gUjdkVm5t3RITXz5NLaPb1cgCLcBGAsYHQ/s1600/1.PNG" /></a></div><br /><br /></div><h3>Double hashing&nbsp;</h3><div>Using a second hashing,&nbsp;</div><div><u>index i = (hash1(k) + step * hash2(k)) mod m</u></div><div><b><span style="color: red;">We have m^2 indexing sequences</span></b></div><div><br /></div><div>Double hashing will allow us to avoid secondary clustering if hash2(k) is relative prime to m</div><div>This also will ensure that we hit all buckets.</div><div><br /></div><div>The average cost of insert (SUHA) and m=n is not O(1) nor O(nlogm) nor O(m+n)</div><div>-&gt; We are depending on the load factor.<br />=&gt; 1/(1-a), a is load factor</div><div><br /></div><div>But the average cost of insert (SUHA) and <b><span style="color: red;">a=0.5</span></b> is <b><span style="color: red;">O(1) </span></b>where a is load factor (How full the hash table is allow to get before max is increase)<br /><br /><br />But double hashing takes more time.</div><div><br /></div><h3>Chaining vs Open addressing</h3><h4>Addressing:</h4><div>- Better memory usage (Cache performance)</div><div>- No references / Dynamic memory allocation</div><div>- Linear addressing&nbsp;</div><div><br /></div><h4>Chaining (Separate):</h4><div>- Less sensitive to hash function</div><div>- Less sensitive to load factors (open addressing degrades past a = 0.7)</div><div><br /></div><div>Average is O(1) but if assumptions are met<br /><br /><h2>Hashing Functions</h2></div><div>What is a good hash function?<br />- Consistent: Same key maps to same buckets<br />- Fast to compute<br />- Scatter the keys into different buckets as uniformly as possible<br /><br /><div><h3>Hash Functions</h3></div><div>We do not need to have an array the size of the universe. We will map actually keys into a bucket</div><div>Hash function takes in a key and map it to some value {0 to m-1} where m is the number of buckets</div><div>Assuming computing h takes O(1) &lt;- this is not true in practice</div><div><br /></div><div><b>Collision:</b></div><div>If two distinct key map to the same bucket.</div><div><br /></div><div><b><span style="color: red;">Pigeonhole principle - There will always be one hole with two pigeons. There will always be a collision if the number of holes are smaller or close to number of pigeons, there will be more collision.</span></b></div><div><b><span style="color: red;"><br /></span></b></div><h4><span style="font-weight: normal;">Adding assumption,</span></h4><div><span style="font-weight: normal;">Assume:</span></div><div><span style="font-weight: normal;">- we have n keys and m is almost equal to n buckets</span></div><div><span style="font-weight: normal;">- keys are uniformly distributed</span></div><div><br /></div><br /><br /><br /></div><div>-&gt; Summing the strings using ascii?</div><div>-&gt; Sum the letters (e.g A-&gt; 1, Z -&gt; 26)? (NO)</div><div><br /></div><div>We want hash function whose values look random, like a random number generators</div><div>Common techniques:</div><div>- Division&nbsp;</div><div>- Multiplication</div><div><br /></div><div>x(n+1) = (aX[n] + c)mod m</div><div>where a, c and m is random numbers</div><h3>Methods</h3><div><b>Division method</b></div><div>h(k) = k mod m<br />m should not share common factors with k as we do not want to skip cells.<br /><br /></div><div><br /></div><div><u>Choice of M:</u></div><div><b><span style="color: red;">How can I use shifts to compute k mod m.</span></b></div><div><b><span style="color: red;">if m = 2^x, because its very fast</span></b></div><div><b><span style="color: red;">k - ((k &gt;&gt; x) &lt;&lt; x)</span></b></div><div>-&gt;We shift right and left to remove the remainder</div><div>-&gt;Take the k and minus the answer to get the lower ordered bits</div><div><br /></div><div><b><span style="color: red;">But does this scatter the keys into different buckets as uniformly as possible?</span></b></div><div><b><span style="color: red;">-&gt; No</span></b></div><div><br /></div><div>=&gt;Reason:</div><div>Imagine what happen if all the input keys are even?</div><div>If all the input keys are even then k mod m must be even.</div><div>k = im + k mod m, where im is some multiple of m</div><div>if k is even, im is even so kmodm is even</div><div>This will cause us to have even pockets and will never hash to an odd pockets.</div><div>-&gt; fast to compute but waste half the space</div><div><br /></div><div><b><span style="color: red;">Assuming chaining, how much of the table do we use if both x and m have a common divsor d?</span></b></div><div><b><span style="color: red;">1/d</span></b></div><div>-&gt; Using the prev answer where even is using half the space ie (1/2), 2 is the divisor for even.</div><div>If its d as divisor then its 1/d<br /><br />Conclusion:<br />- Choose m to be Prime, avoid powers of 2 and 10<br />- Not always effective<br />- Slow (no more shift)&nbsp;</div><div><br /></div><h4>Multiplication method</h4><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-btVcCIDKMu0/XYR3U9qJC1I/AAAAAAAACG8/BAkptK0Z9qI7bLUGUHRTeFtn43UuifMzgCLcBGAsYHQ/s1600/5.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="410" data-original-width="400" height="320" src="https://1.bp.blogspot.com/-btVcCIDKMu0/XYR3U9qJC1I/AAAAAAAACG8/BAkptK0Z9qI7bLUGUHRTeFtn43UuifMzgCLcBGAsYHQ/s320/5.PNG" width="312" /></a></div><div><br /></div><div>Table size: m = 2^r</div><div>Constant: 2^(w-1) &lt; A &lt; 2^w</div><div>h(k) = (Ak) mod 2^w &gt;&gt; (w-r) , w is size of keys in bits , r from table size, A is constant</div><div><b><span style="color: red;">We can't let A to be even because we will be multiplying a shift</span></b></div><div>=&gt; Odd * even = even<br />=&gt; The last few digits will be 0<br />=&gt; Cause at least one bit of information loss</div><h3>Table Size</h3><div>Assumption:</div><div>- Hashing with chaining</div><div>- Simple uniform Hashing</div><div>- Expected search O(1+n/m)</div><div>- Optimal Size: m = cn</div><div>m &lt; 2n : too many collision</div><div>m&gt; 10n : too much space</div><div>But we don't know n in advance</div><div><br /></div><h4>Dynamic Table size</h4><div>Grow and Shrink the table size as necessary</div><div><br /></div><div><b><span style="color: red;">We cannot just copy the old table to new table because the hashing uses the table size.</span></b></div><div><br /></div><div>1. Choose new table size</div><div>2. Choose new hash function h2 that matches m2</div><div>3. For each key-value pair ki,vi in the old hash table</div><div>&nbsp; &nbsp; - Compute new bucket b2 = h2(ki)</div><div>&nbsp; &nbsp; - Insert (ki, Vi) into bucket b2</div><div><br /></div><div><span class="fontstyle0">Costs:</span><span class="fontstyle0">- Creating new table: O(m2)</span><span class="fontstyle2"><br style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: normal; text-align: -webkit-auto; text-size-adjust: auto;" /></span></div><div><span class="fontstyle0">- Scanning Old hash table: O(m1)</span></div><div><span class="fontstyle0">- Insert all n elements into new table : O(n)</span></div><div>Total: O(m1+m2+n)</div><div><br /></div><h4>Increment by 1</h4><div>If table is not full, ci = 1</div><div>If n = m (table is full) , then expand so ci = i [new table]+ (i - 1) [old table] + 1 [insert] = 2i</div><div>// we need to copy over, expand and calculate more</div><div><br /></div><div><b><span style="color: red;">What is the total cost to insert n elements into the table? O(n^2)</span></b></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-ezh2MxFXBSA/XYG3Ks830GI/AAAAAAAACGQ/wqdDPpJNxQs91a5wcARYFMIqn_KfU2p9gCLcBGAsYHQ/s1600/1.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="222" data-original-width="289" src="https://1.bp.blogspot.com/-ezh2MxFXBSA/XYG3Ks830GI/AAAAAAAACGQ/wqdDPpJNxQs91a5wcARYFMIqn_KfU2p9gCLcBGAsYHQ/s1600/1.PNG" /></a></div><div><br /></div><div>We use<br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-5gfD77GS0vM/XYG3YW-6I5I/AAAAAAAACGU/UjD8ykPwkukC78v-5BYHKm_BQpGu02fiwCLcBGAsYHQ/s1600/2.PNG" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" data-original-height="33" data-original-width="165" src="https://1.bp.blogspot.com/-5gfD77GS0vM/XYG3YW-6I5I/AAAAAAAACGU/UjD8ykPwkukC78v-5BYHKm_BQpGu02fiwCLcBGAsYHQ/s1600/2.PNG" /></a></div></div><div><br /></div><div>to plug in.</div><div><br /></div><div><b><span style="color: red;">But the average for insert (amortized) is O(n)</span></b></div><div>(Total cost divided by the number if elements which is n)<br />Amortized-&gt; totalcost/ numberofElements</div><div><br /></div><div><b>Doubling the size</b></div><div>If n&lt;m, ci = 1</div><div>If n = m (table is full), then expand so , ci = 2i + (i-1) + 1 =3i</div><div>&nbsp; &nbsp; - table is full only when i is an exact power of 2</div><div><br /></div><div><u><b><span style="color: red;">The total cost to insert n elements is O(n),</span></b></u><br /><u><b><span style="color: red;">The amortized cost for insert is O(1)</span></b></u></div><div>j is the number of resizing we need,</div><div>Resize each time we reach full then we double it</div><div>2^ j = n</div><div>j = log[2]n</div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-79kcewib_Zk/XYG4UPb5k0I/AAAAAAAACGk/vEj6K2nEIWwvqyaW9IrJwJBA0uTlBx-AwCLcBGAsYHQ/s1600/3.PNG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="196" data-original-width="266" src="https://1.bp.blogspot.com/-79kcewib_Zk/XYG4UPb5k0I/AAAAAAAACGk/vEj6K2nEIWwvqyaW9IrJwJBA0uTlBx-AwCLcBGAsYHQ/s1600/3.PNG" /></a></div><div>Using the method<br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-tetJkCc9-kc/XYG4ZT2K56I/AAAAAAAACGo/EJCJR5vualg-QXitbEMKU62D0M_pkVOAACLcBGAsYHQ/s1600/4.PNG" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" data-original-height="33" data-original-width="139" src="https://1.bp.blogspot.com/-tetJkCc9-kc/XYG4ZT2K56I/AAAAAAAACGo/EJCJR5vualg-QXitbEMKU62D0M_pkVOAACLcBGAsYHQ/s1600/4.PNG" /></a></div></div><div><br /></div><div><br /></div><div><br /></div><div>Average is O(1)</div><div><br /></div><div>In conclusion, we use double the size</div><div><br /></div><div>Cost of resizing including worst case insertion: O(n)</div><div>But on average is O(1)</div><div><br /><h3>Square the size</h3></div><div>Cost of inserting n elements: O(n^2)</div><div>Cost of inserting (amortized) n elements: O(n)</div><h3>Amortized Analysis: Accounting Method</h3><div><span class="fontstyle0">Operation has </span><span class="fontstyle2">amortized cost T(n)</span><span class="fontstyle3">&nbsp;if for every integer k, the cost of k Operation is &lt;= kT(n)</span></div><span class="fontstyle2">  </span><br /><div><span class="fontstyle2"><span class="fontstyle2"><br /></span></span></div><span class="fontstyle2">When we perform an operation, it cost something to put in.</span><br /><div><span class="fontstyle2">- We will have a charge amount for each insert</span></div><div><span class="fontstyle2">- Ensure bank account is always big enough to pay for resizing</span></div><div><span class="fontstyle2"><br /></span></div><div><span class="fontstyle2"><br /></span></div><div><span class="fontstyle2"><br style="font-variant-east-asian: normal; font-variant-numeric: normal; line-height: normal; text-align: -webkit-auto; text-size-adjust: auto;" /></span><br /><div><span class="fontstyle2"><br /></span></div></div>